{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "EDA_Tutorial_Colab.ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploratory Data Analysis (EDA) Tutorial — Google Colab\n",
        "\n",
        "_Generated on 2025-10-28 13:30 UTC_\n",
        "\n",
        "This notebook guides you through a practical, step-by-step EDA workflow in Google Colab.\n",
        "It uses only standard scientific Python libraries and **matplotlib** for charts.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup_imports"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Install (if needed) and import dependencies\n",
        "!pip -q install pandas numpy matplotlib scipy\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "import io, textwrap\n",
        "\n",
        "pd.set_option('display.max_columns', 100)\n",
        "pd.set_option('display.width', 120)\n",
        "print(pd.__version__, np.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Your Data\n",
        "Choose one of the following approaches to load a dataset:\n",
        "1) From a URL, 2) Upload from your computer, or 3) From Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "load_data"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from typing import Optional\n",
        "\n",
        "# Option A: From a URL\n",
        "URL: Optional[str] = ''  # e.g., 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv'\n",
        "\n",
        "df = None\n",
        "if URL:\n",
        "    df = pd.read_csv(URL)\n",
        "\n",
        "# Option B: Upload from local machine\n",
        "if df is None:\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        uploaded = files.upload()\n",
        "        for fname in uploaded.keys():\n",
        "            if fname.lower().endswith(('.csv', '.txt')):\n",
        "                df = pd.read_csv(io.BytesIO(uploaded[fname]))\n",
        "                break\n",
        "            elif fname.lower().endswith(('.xlsx', '.xls')):\n",
        "                df = pd.read_excel(io.BytesIO(uploaded[fname]))\n",
        "                break\n",
        "    except Exception as e:\n",
        "        print('Upload skipped or not in Colab:', e)\n",
        "\n",
        "# Option C: Load from Google Drive\n",
        "if df is None:\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        # Example path: '/content/drive/MyDrive/path/to/data.csv'\n",
        "        DRIVE_PATH = ''\n",
        "        if DRIVE_PATH:\n",
        "            if DRIVE_PATH.lower().endswith(('.csv', '.txt')):\n",
        "                df = pd.read_csv(DRIVE_PATH)\n",
        "            else:\n",
        "                df = pd.read_excel(DRIVE_PATH)\n",
        "    except Exception as e:\n",
        "        print('Drive not mounted:', e)\n",
        "\n",
        "assert df is not None, 'No dataset loaded. Please set URL, upload a file, or provide DRIVE_PATH.'\n",
        "print('Loaded shape:', df.shape)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick Glance at the Data\n",
        "Check the shape, columns, info, and first/last rows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quick_glance"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "print('Shape:', df.shape)\n",
        "print('\\nColumns:', list(df.columns))\n",
        "print('\\nInfo:')\n",
        "df.info()\n",
        "display(df.head())\n",
        "display(df.tail())\n",
        "display(df.sample(min(5, len(df))))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Memory Usage and Dtypes\n",
        "Identify potential optimizations for large datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "memory_types"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "mem = df.memory_usage(deep=True).sum()/1024**2\n",
        "print(f'Total memory usage: {mem:.3f} MB')\n",
        "display(df.dtypes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Missing Values Overview\n",
        "Count and percent missing per column, and visualize the missingness pattern."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "missing_values"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "na_count = df.isna().sum().sort_values(ascending=False)\n",
        "na_pct = (df.isna().mean()*100).sort_values(ascending=False)\n",
        "missing_summary = pd.DataFrame({'missing_count': na_count, 'missing_pct': na_pct})\n",
        "display(missing_summary)\n",
        "\n",
        "# Visualize missing matrix using matplotlib\n",
        "plt.figure()\n",
        "plt.imshow(df.isna(), aspect='auto', interpolation='nearest')\n",
        "plt.title('Missing Data Pattern (rows x columns)')\n",
        "plt.xlabel('Columns')\n",
        "plt.ylabel('Rows')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Duplicate Rows\n",
        "Identify and optionally drop duplicates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duplicates"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "dupe_count = df.duplicated().sum()\n",
        "print('Duplicate rows:', dupe_count)\n",
        "# Uncomment to drop\n",
        "# df = df.drop_duplicates().reset_index(drop=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Descriptive Statistics\n",
        "Summary stats for numeric and categorical variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "descriptive_stats"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "display(df.describe(include=[np.number]).T)\n",
        "display(df.describe(include=['object', 'category']).T)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cardinality and Top Categories\n",
        "Check unique counts and most frequent values for categorical columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cardinality_topcats"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "summary = []\n",
        "for c in cat_cols:\n",
        "    vc = df[c].value_counts(dropna=False)\n",
        "    summary.append({'column': c, 'unique': df[c].nunique(dropna=False), 'top_value': vc.index[0] if len(vc)>0 else None, 'top_count': int(vc.iloc[0]) if len(vc)>0 else 0})\n",
        "display(pd.DataFrame(summary))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Distributions — Histograms (Numeric)\n",
        "Plot histograms for each numeric column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "numeric_hist"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "for col in num_cols:\n",
        "    plt.figure()\n",
        "    plt.hist(df[col].dropna(), bins=30)\n",
        "    plt.title(f'Histogram: {col}')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Distributions — Boxplots (Numeric)\n",
        "Visualize spread and potential outliers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "numeric_box"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "for col in num_cols:\n",
        "    if df[col].dropna().shape[0] > 0:\n",
        "        plt.figure()\n",
        "        plt.boxplot(df[col].dropna(), vert=True)\n",
        "        plt.title(f'Boxplot: {col}')\n",
        "        plt.ylabel(col)\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Categorical Frequencies — Top 15\n",
        "Bar charts for the most frequent categories (per column)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "categorical_bars"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "TOP_N = 15\n",
        "for col in cat_cols:\n",
        "    vc = df[col].astype('object').fillna('NA').value_counts().head(TOP_N)\n",
        "    plt.figure()\n",
        "    plt.bar(vc.index.astype(str), vc.values)\n",
        "    plt.title(f'Top {TOP_N} Categories: {col}')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.ylabel('Count')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Correlation Matrix (Numeric)\n",
        "Compute and visualize correlations using matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "correlation_matrix"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "if len(num_cols) >= 2:\n",
        "    corr = df[num_cols].corr(numeric_only=True)\n",
        "    plt.figure()\n",
        "    plt.imshow(corr, cmap=None)\n",
        "    plt.colorbar()\n",
        "    plt.xticks(range(len(num_cols)), num_cols, rotation=90)\n",
        "    plt.yticks(range(len(num_cols)), num_cols)\n",
        "    plt.title('Correlation Matrix')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    display(corr)\n",
        "else:\n",
        "    print('Not enough numeric columns for correlation matrix.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pairwise Scatter (Sampled)\n",
        "For small datasets, this gives a quick view of linear relationships. For large datasets, we sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pairwise_scatter"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from itertools import combinations\n",
        "max_points = 2000\n",
        "sample = df.sample(n=min(len(df), max_points), random_state=42) if len(df) > max_points else df.copy()\n",
        "pairs = list(combinations(num_cols[:5], 2))  # limit to first 5 numeric cols for brevity\n",
        "for x, y in pairs:\n",
        "    plt.figure()\n",
        "    plt.scatter(sample[x], sample[y], s=10)\n",
        "    plt.xlabel(x)\n",
        "    plt.ylabel(y)\n",
        "    plt.title(f'Scatter: {x} vs {y}')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Outlier Detection (IQR Method)\n",
        "Identify outliers per numeric column using the 1.5×IQR rule."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "outliers_iqr"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "def iqr_outliers(series: pd.Series):\n",
        "    q1, q3 = np.percentile(series.dropna(), [25, 75])\n",
        "    iqr = q3 - q1\n",
        "    lower = q1 - 1.5*iqr\n",
        "    upper = q3 + 1.5*iqr\n",
        "    mask = (series < lower) | (series > upper)\n",
        "    return mask, lower, upper\n",
        "\n",
        "outlier_summary = []\n",
        "for col in num_cols:\n",
        "    mask, lower, upper = iqr_outliers(df[col])\n",
        "    outlier_summary.append({'column': col, 'outlier_count': int(mask.sum()), 'lower_bound': lower, 'upper_bound': upper})\n",
        "display(pd.DataFrame(outlier_summary))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: Define a Target Column\n",
        "If you have a target variable (e.g., for classification/regression), set it here for focused EDA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "target_var"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "TARGET = ''  # e.g., 'Survived' for Titanic\n",
        "if TARGET and TARGET in df.columns:\n",
        "    print('Target dtype:', df[TARGET].dtype)\n",
        "    if pd.api.types.is_numeric_dtype(df[TARGET]):\n",
        "        plt.figure()\n",
        "        plt.hist(df[TARGET].dropna(), bins=30)\n",
        "        plt.title(f'Target Distribution: {TARGET}')\n",
        "        plt.xlabel(TARGET)\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.show()\n",
        "    else:\n",
        "        vc = df[TARGET].astype('object').fillna('NA').value_counts()\n",
        "        plt.figure()\n",
        "        plt.bar(vc.index.astype(str), vc.values)\n",
        "        plt.title(f'Target Classes: {TARGET}')\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.ylabel('Count')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "else:\n",
        "    print('No TARGET set or not found in columns.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Numeric Columns vs Target (if defined)\n",
        "Group summaries to compare distributions by target class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "numeric_vs_target"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "if TARGET and TARGET in df.columns and not pd.api.types.is_numeric_dtype(df[TARGET]):\n",
        "    for col in num_cols:\n",
        "        grp = df[[TARGET, col]].dropna().groupby(TARGET)[col].describe()\n",
        "        print(f\"\\n{col} by {TARGET}\")\n",
        "        display(grp)\n",
        "else:\n",
        "    print('No categorical TARGET set for grouped numeric summaries.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: Time Series Quick Look\n",
        "If you have a date column, set it and view a simple trend."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "time_series"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "DATE_COL = ''  # e.g., 'date'\n",
        "VALUE_COL = '' # a numeric column to aggregate\n",
        "if DATE_COL and DATE_COL in df.columns and VALUE_COL and VALUE_COL in df.columns:\n",
        "    tmp = df[[DATE_COL, VALUE_COL]].dropna().copy()\n",
        "    tmp[DATE_COL] = pd.to_datetime(tmp[DATE_COL], errors='coerce')\n",
        "    tmp = tmp.dropna(subset=[DATE_COL])\n",
        "    tmp = tmp.set_index(DATE_COL).sort_index()\n",
        "    ts = tmp[VALUE_COL].resample('D').mean()\n",
        "    plt.figure()\n",
        "    plt.plot(ts.index, ts.values)\n",
        "    plt.title(f'Daily Mean of {VALUE_COL}')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel(VALUE_COL)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print('No DATE_COL/VALUE_COL set for time series section.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: Simple Feature Engineering\n",
        "Create a few example features (log transforms, simple ratios)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feature_eng"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "ENGINEERED_FEATURES = []\n",
        "for col in num_cols:\n",
        "    if (df[col] > 0).all():\n",
        "        df[f'log_{col}'] = np.log(df[col])\n",
        "        ENGINEERED_FEATURES.append(f'log_{col}')\n",
        "print('Engineered features:', ENGINEERED_FEATURES)\n",
        "display(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export a Clean Snapshot\n",
        "Save a working copy of the current dataframe to CSV."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "export_clean"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "OUTPUT_CSV = 'eda_clean_snapshot.csv'\n",
        "df.to_csv(OUTPUT_CSV, index=False)\n",
        "print('Saved:', OUTPUT_CSV)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "- Explore relationships between variables more deeply.\n",
        "- Consider statistical tests (t-tests, chi-square) appropriate to your data.\n",
        "- Build baseline models (where relevant) for predictive tasks.\n"
      ]
    }
  ]
}